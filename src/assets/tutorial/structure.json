[{"name": "t01_basic_usage.md", "displayName": " Tutorial 1: Getting started with FastEstimator", "toc": ["Tutorial 1: Getting started with FastEstimator", "Step 1: Prepare the Pipeline", "Import training and validation data as numpy array for instance", "Add one channel dimension for convolution later", "Create a dictionary to identify the training and evaluation data", "We specify for each x images and y label also in a dictionnary", "Creating the pipeline with the desired batchsize and preprocessing operation here Minmax", "Step 2: Define the network", "We first define a model using FEModel to compile it", "We summarize all operations and loss in the Network", "Step 3: Create the Estimator", "We create the estimator and specify the number of epochs for training", "and train your model"], "toc_route": ["tutorial_1_getting_started_with_fastestimator", "step_1_prepare_the_pipeline", "import_training_and_validation_data_as_numpy_array_for_instance", "add_one_channel_dimension_for_convolution_later", "create_a_dictionary_to_identify_the_training_and_evaluation_data", "we_specify_for_each_x_images_and_y_label_also_in_a_dictionnary", "creating_the_pipeline_with_the_desired_batchsize_and_preprocessing_operation_here_minmax", "step_2_define_the_network", "we_first_define_a_model_using_femodel_to_compile_it", "we_summarize_all_operations_and_loss_in_the_network", "step_3_create_the_estimator", "we_create_the_estimator_and_specify_the_number_of_epochs_for_training", "and_train_your_model"]}, {"name": "t02_using_data_in_disk.md", "displayName": " Tutorial 2: Dealing with large datasets with FastEstimator", "toc": ["Tutorial 2: Dealing with large datasets with FastEstimator", "Before we start:", "Step 0: Get the paths to the csv files", "Step 1: RecordWriter", "We simply create a RecordWriter will all required arguments", "Step 2: Pipeline  Network  Estimator see tutorial 1 for details", "Pipeline creation", "Model and network definition", "Estimator definition", "Step 3: Start training", "Launch the training", "Key takeaways:"], "toc_route": ["tutorial_2_dealing_with_large_datasets_with_fastestimator", "before_we_start", "step_0_get_the_paths_to_the_csv_files", "step_1_recordwriter", "we_simply_create_a_recordwriter_will_all_required_arguments", "step_2_pipeline__network__estimator_see_tutorial_1_for_details", "pipeline_creation", "model_and_network_definition", "estimator_definition", "step_3_start_training", "launch_the_training", "key_takeaways"]}, {"name": "t03_operator.md", "displayName": " Tutorial 3: Operator", "toc": ["Tutorial 3: Operator", "How does Operator work", "How to express Operator connections in FastEstimator", "What different types of Operators are there", "How is an Operator defined", "Operator demo in FastEstimator", "Import libraries", "Download data in a temporary repository using loaddata", "Step 0: Use prebuilt Op and custom Op for data preprocessing in RecordWriter", "Create a custom Numpy Op to rescale images in forward function", "Define the RecordWriter with two ops Rescale and predefined ImageReader", "Step 1: Use prebuilt and custom Ops for Pipeline", "Create a custom Resize Tensor op", "We need init here as we want to add the size argument", "Create Pipeline with Resize op and Augmentation prebuilt op", "Augmentation2D automatically augment the dataset with rotation in the specified range", "Step 2: Use prebuilt and custom ops for Network", "Create a custom TensorOp", "Build the model and network", "Step 3: Create the Estimator and train", "Create the estimator", "Launch the training"], "toc_route": ["tutorial_3_operator", "how_does_operator_work", "how_to_express_operator_connections_in_fastestimator", "what_different_types_of_operators_are_there", "how_is_an_operator_defined", "operator_demo_in_fastestimator", "import_libraries", "download_data_in_a_temporary_repository_using_loaddata", "step_0_use_prebuilt_op_and_custom_op_for_data_preprocessing_in_recordwriter", "create_a_custom_numpy_op_to_rescale_images_in_forward_function", "define_the_recordwriter_with_two_ops_rescale_and_predefined_imagereader", "step_1_use_prebuilt_and_custom_ops_for_pipeline", "create_a_custom_resize_tensor_op", "we_need_init_here_as_we_want_to_add_the_size_argument", "create_pipeline_with_resize_op_and_augmentation_prebuilt_op", "augmentation2d_automatically_augment_the_dataset_with_rotation_in_the_specified_range", "step_2_use_prebuilt_and_custom_ops_for_network", "create_a_custom_tensorop", "build_the_model_and_network", "step_3_create_the_estimator_and_train", "create_the_estimator", "launch_the_training"]}, {"name": "t04_pipeline_debug_benchmark.md", "displayName": " Tutorial 4: Pipeline debugging and benchmarking", "toc": ["Tutorial 4: Pipeline debugging and benchmarking", "1 Define the pipeline same as tutorial 3", "Create Rescale and Resize custom ops", "Load data", "Create RecordWriter", "Create Pipeline", "2 Access the pipeline results", "Use showresults by specifying the epoch mode and step batch", "Isolate x and y from result", "Display 4 examples of data after Pipeline", "3 Benchmark pipeline speed", "You just have to specify the epoch mode and number of batches"], "toc_route": ["tutorial_4_pipeline_debugging_and_benchmarking", "1_define_the_pipeline_same_as_tutorial_3", "create_rescale_and_resize_custom_ops", "load_data", "create_recordwriter", "create_pipeline", "2_access_the_pipeline_results", "use_showresults_by_specifying_the_epoch_mode_and_step_batch", "isolate_x_and_y_from_result", "display_4_examples_of_data_after_pipeline", "3_benchmark_pipeline_speed", "you_just_have_to_specify_the_epoch_mode_and_number_of_batches"]}, {"name": "t05_trace_debug_training.md", "displayName": " Tutorial 5: Trace  training control and debugging", "toc": ["Tutorial 5: Trace  training control and debugging", "Import libraries", "Using Trace to debug training loop", "1 Define the operation to test  pipeline and network", "We define the scaling operation", "We load data create dictionnaries and prepare the Pipeline", "We prepare the model and network which will use the scaling operation", "2 Define the trace", "We define a trace to show the predictions and test the scaling op", "We finally define the estimator specifying the trace argument For debugging we only use one epoch with one step", "We launch the training and can see what the scaled prediction looks like"], "toc_route": ["tutorial_5_trace__training_control_and_debugging", "import_libraries", "using_trace_to_debug_training_loop", "1_define_the_operation_to_test__pipeline_and_network", "we_define_the_scaling_operation", "we_load_data_create_dictionnaries_and_prepare_the_pipeline", "we_prepare_the_model_and_network_which_will_use_the_scaling_operation", "2_define_the_trace", "we_define_a_trace_to_show_the_predictions_and_test_the_scaling_op", "we_finally_define_the_estimator_specifying_the_trace_argument_for_debugging_we_only_use_one_epoch_with_one_step", "we_launch_the_training_and_can_see_what_the_scaled_prediction_looks_like"]}, {"name": "t06_TensorFilter_imbalanced_training.md", "displayName": " Tutorial 6:  Dealing with imbalanced dataset using TensorFilter", "toc": ["Tutorial 6:  Dealing with imbalanced dataset using TensorFilter", "Step 0  Data preparation same as tutorial 1", "Import libraries", "Load data and create dictionaries", "Step 1  Customize your own Filter", "We create our filter in forward function its just our condition", "We specify the filter in Pipeline ops list", "Lets check our pipeline ops results with showresults", "or use a prebuilt ScalarFilter", "We specify the list of scalars to filter out and the probability to keep these scalars", "Lets check our pipeline ops results with showresults"], "toc_route": ["tutorial_6__dealing_with_imbalanced_dataset_using_tensorfilter", "step_0__data_preparation_same_as_tutorial_1", "import_libraries", "load_data_and_create_dictionaries", "step_1__customize_your_own_filter", "we_create_our_filter_in_forward_function_its_just_our_condition", "we_specify_the_filter_in_pipeline_ops_list", "lets_check_our_pipeline_ops_results_with_showresults", "or_use_a_prebuilt_scalarfilter", "we_specify_the_list_of_scalars_to_filter_out_and_the_probability_to_keep_these_scalars", "lets_check_our_pipeline_ops_results_with_showresults"]}, {"name": "t07_expand_data_dimension.md", "displayName": " Tutorial 7: Expanding data dimension in RecordWriter and Pipeline", "toc": ["Tutorial 7: Expanding data dimension in RecordWriter and Pipeline", "Import libraries", "Step 1  RecordWriter: expand data dimension and write it to the disk", "Load Mnist data", "Create a custom Numpy op to sample 4 images from the corners of each image", "we sample 4 27x27 images from the corners:", "We insert this custom op in the ops list of RecordWriter", "We have to specify expanddimsTrue to allow data dimension expansion", "Step 2  Pipeline: expand dimension on the fly", "We create a custom op for random sampling", "We randomly select the topleft point of our image for each sample x and y coordinate", "It cannot be greater than 8 as we will sample a 20x20 image from a 27x27 one", "We sample two 20x20 images with x1y1 and x2y2 topleft corner", "Create Pipeline with RandomSample op and expanddimsTrue", "Step 3  Check pipeline results", "Lets check our pipeline ops results with showresults", "and visualize", "Lets visualize the first 4 images keeping the order from our postpipeline data:"], "toc_route": ["tutorial_7_expanding_data_dimension_in_recordwriter_and_pipeline", "import_libraries", "step_1__recordwriter_expand_data_dimension_and_write_it_to_the_disk", "load_mnist_data", "create_a_custom_numpy_op_to_sample_4_images_from_the_corners_of_each_image", "we_sample_4_27x27_images_from_the_corners", "we_insert_this_custom_op_in_the_ops_list_of_recordwriter", "we_have_to_specify_expanddimstrue_to_allow_data_dimension_expansion", "step_2__pipeline_expand_dimension_on_the_fly", "we_create_a_custom_op_for_random_sampling", "we_randomly_select_the_topleft_point_of_our_image_for_each_sample_x_and_y_coordinate", "it_cannot_be_greater_than_8_as_we_will_sample_a_20x20_image_from_a_27x27_one", "we_sample_two_20x20_images_with_x1y1_and_x2y2_topleft_corner", "create_pipeline_with_randomsample_op_and_expanddimstrue", "step_3__check_pipeline_results", "lets_check_our_pipeline_ops_results_with_showresults", "and_visualize", "lets_visualize_the_first_4_images_keeping_the_order_from_our_postpipeline_data"]}, {"name": "t08_scheduler_progressive_training.md", "displayName": " Tutorial 8: Changing hyperparameters during training with Scheduler", "toc": ["Tutorial 8: Changing hyperparameters during training with Scheduler", "1 How to use Scheduler:", "2 Scheduler example:", "Step 0 Prepare data", "We load MNIST dataset", "Step 1 Prepare the Pipeline with the Schedulers", "We create a scheduler for batchsize with the epochs at which it will change and corresponding values", "We create a scheduler for the Resize ops", "We create a scheduler for the different normalize ops we will want to use", "In Pipeline we use the schedulers for batchsize and ops", "Step 2 Prepare Network with the two models and a Scheduler", "We create two models and build them with their optimizer and loss", "We create a Scheduler to indicate what model we want to train for each epoch", "We summarize the ops in Network using modelscheduler for ModelOp", "Step 3 Build the Estimator and train"], "toc_route": ["tutorial_8_changing_hyperparameters_during_training_with_scheduler", "1_how_to_use_scheduler", "2_scheduler_example", "step_0_prepare_data", "we_load_mnist_dataset", "step_1_prepare_the_pipeline_with_the_schedulers", "we_create_a_scheduler_for_batchsize_with_the_epochs_at_which_it_will_change_and_corresponding_values", "we_create_a_scheduler_for_the_resize_ops", "we_create_a_scheduler_for_the_different_normalize_ops_we_will_want_to_use", "in_pipeline_we_use_the_schedulers_for_batchsize_and_ops", "step_2_prepare_network_with_the_two_models_and_a_scheduler", "we_create_two_models_and_build_them_with_their_optimizer_and_loss", "we_create_a_scheduler_to_indicate_what_model_we_want_to_train_for_each_epoch", "we_summarize_the_ops_in_network_using_modelscheduler_for_modelop", "step_3_build_the_estimator_and_train"]}, {"name": "t09_learning_rate_controller.md", "displayName": " Tutorial 9: Learning Rate Controller", "toc": ["Tutorial 9: Learning Rate Controller", "Step 0 Preparation", "Create a function to get Pipeline and Network", "step 1 Prepare data", "step 2 Prepare model", "Option 1 Customize the learning rate: stepwise control", "Create a LR Scheduler with a custom schedulefn", "Create pipeline network and lrscheduler", "In Estimator indicate in traces the LR Scheduler using LR Controller you also have to specify the modelname", "Save the training history and train the model", "Show the learning rates history for each step", "Option 2  Customize the learning rate: epochwise control", "We define our custom Scheduler in the same way as above", "Create pipeline and network", "Here we now indicate epoch as schedulemode", "Train and save history", "Show the learning rate for each step: it changes only at an epoch level", "Option 3 Builtin Cyclic Learning Rate  example 1", "Create pipeline and network", "Directly use the prebuilt CyclicLRSchedule with a cosine decrease method and one cycle", "Train and save history", "Plot the learning rate for each step", "Option 3 Builtin Cyclic Learning Rate: example 2", "We create pipeline and network", "We specify numcycle and cyclemultiplier in CyclicLRSchedule", "Train and save history", "Plot the learning rate"], "toc_route": ["tutorial_9_learning_rate_controller", "step_0_preparation", "create_a_function_to_get_pipeline_and_network", "step_1_prepare_data", "step_2_prepare_model", "option_1_customize_the_learning_rate_stepwise_control", "create_a_lr_scheduler_with_a_custom_schedulefn", "create_pipeline_network_and_lrscheduler", "in_estimator_indicate_in_traces_the_lr_scheduler_using_lr_controller_you_also_have_to_specify_the_modelname", "save_the_training_history_and_train_the_model", "show_the_learning_rates_history_for_each_step", "option_2__customize_the_learning_rate_epochwise_control", "we_define_our_custom_scheduler_in_the_same_way_as_above", "create_pipeline_and_network", "here_we_now_indicate_epoch_as_schedulemode", "train_and_save_history", "show_the_learning_rate_for_each_step_it_changes_only_at_an_epoch_level", "option_3_builtin_cyclic_learning_rate__example_1", "create_pipeline_and_network", "directly_use_the_prebuilt_cycliclrschedule_with_a_cosine_decrease_method_and_one_cycle", "train_and_save_history", "plot_the_learning_rate_for_each_step", "option_3_builtin_cyclic_learning_rate_example_2", "we_create_pipeline_and_network", "we_specify_numcycle_and_cyclemultiplier_in_cycliclrschedule", "train_and_save_history", "plot_the_learning_rate"]}, {"name": "t10_unpaired_dataset.md", "displayName": " Tutorial 10: Dataset with unpaired features", "toc": ["Tutorial 10: Dataset with unpaired features", "Step 0  Data preparation and visualization", "Use loaddata from our dataset API to load the dataset", "Lets take a look at the data by loading the csv file with all images path information", "We select one image of horse and one of zebra and plot them", "Step 1  RecordWriter: read unpaired features using a tuple", "Create a RecordWriter with a tuple of two ops to pair images", "We write the data to the disk using the write method"], "toc_route": ["tutorial_10_dataset_with_unpaired_features", "step_0__data_preparation_and_visualization", "use_loaddata_from_our_dataset_api_to_load_the_dataset", "lets_take_a_look_at_the_data_by_loading_the_csv_file_with_all_images_path_information", "we_select_one_image_of_horse_and_one_of_zebra_and_plot_them", "step_1__recordwriter_read_unpaired_features_using_a_tuple", "create_a_recordwriter_with_a_tuple_of_two_ops_to_pair_images", "we_write_the_data_to_the_disk_using_the_write_method"]}, {"name": "t11_interpretation.md", "displayName": " Tutorial 11: Interpretation", "toc": ["Tutorial 11: Interpretation", "Download a sample model for demonstration", "Interpretation with Bash", "Interpretation with Python API", "Interpretation with Traces"], "toc_route": ["tutorial_11_interpretation", "download_a_sample_model_for_demonstration", "interpretation_with_bash", "interpretation_with_python_api", "interpretation_with_traces"]}]